{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data which I chose for this project is the one from Zindi Africa hackathon\n",
    "#organized by Data science Nigeria on predicting Insurance claim based on data from select buildings\n",
    "#We have 7,161 observations \n",
    "# link https://zindi.africa/competitions/data-science-nigeria-2019-challenge-1-insurance-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'hyperdrive insurance'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "\n",
    "# Get the data of Kaggle Titanic Dataset\n",
    "key = \"insurance data\"\n",
    "description_text = \"Data from Zindi Africa Data hackathon\"\n",
    "found = False\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "    found = True\n",
    "    dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "    # Create AML Dataset and register it into Workspace\n",
    "    example_data = 'https://raw.githubusercontent.com/Ogbuchi-Ikechukwu/Azure-ML-Nanodegree-Capstone/master/starter_file/train_data_cleaned.csv'\n",
    "    dataset = Dataset.Tabular.from_delimited_files(example_data)\n",
    "    #Register Dataset in Workspace\n",
    "    dataset = dataset.register(workspace=ws,\n",
    "                               name=key,\n",
    "                               description=description_text)\n",
    "\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating compute for hyper drive\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "# TODO: Create compute cluster\n",
    "# max_nodes should be no greater than 4.\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"drive-compute\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS3_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=30)\n",
    "    \n",
    " # use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependencies file for the train script\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy('config.json', project_folder)\n",
    "            \n",
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n",
    "\n",
    "Here I am using a Logistic Regression model imported from  SKLearn framework to predict whether a given building based on its features will make an insurance claim over a given period of time. Hyperdrive is used to sample different values for two chosen algorithm hyperparameters:\n",
    "\n",
    "\"C\": Inverse of regularization strength and \"max_iter\": representing the Maximum number of iterations taken for the solvers to converge. Different combinations of these hyperparameters will lead to different results and the goal is to find the best values.\n",
    "\n",
    "I have chosen the Random Sampler because it enables me randomly select parameters from a given range. My choice for C is between 0.002 and 1.00. For max_iter a made a choice of 50, 150, 1000\n",
    "\n",
    "My choice here was to sample the values using Random Sampling, in which hyperparameter values are randomly selected from the defined search space. \"C\" is chosen randomly in uniformly distributed between 0.001 and 1.0, while \"max_iter\" is sampled from one of the three values: 1000, 10000 and 100000.\n",
    "\n",
    "Bandit Policy was chosen and set at 0.1, which ensures that any run not within the slack factor of the evaluation metric (in our case, \"Accuracy\") with respect to the best performing run is terminated. \n",
    "\n",
    "A training script train.py has been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create an early termination policy. \n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1)\n",
    "\n",
    "# Create the different params that you will be using during training\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    '--C': uniform(0.002, 1.0),\n",
    "    '--max_iter': choice(50, 150, 1000)\n",
    "} )\n",
    "\n",
    "# Create your estimator and hyperdrive config\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='train.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=sklearn_env)\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=param_sampling,\n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name=\"Accuracy\",\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Submit your experiment\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(best_run.get_details()['runId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics\n",
    "print(best_run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model \n",
    "best_run.download_file('outputs/model.joblib', '/model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model\n",
    "model = best_run.register_model(model_name='sklearn-titanic',\n",
    "                                model_path='outputs/model.joblib',\n",
    "                                model_framework=Model.Framework.SCIKITLEARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = 'ikechukwu-service'\n",
    "service = Model.deploy(ws, service_name, [model])\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\"data\":\n",
    "        [\n",
    "          {\n",
    "            \"PassengerId\": 812,\n",
    "            \"Pclass\": 2,\n",
    "            \"Age\": 23.0,\n",
    "            \"SibSp\": 0,\n",
    "            \"Parch\": 0, \n",
    "            \"Fare\": 13.0,\n",
    "            \"Q\": 0,\n",
    "            \"S\": 1,\n",
    "            \"male\": 1\n",
    "          },\n",
    "          {\n",
    "            \"PassengerId\": 813,\n",
    "            \"Pclass\": 1,\n",
    "            \"Age\": 35.0,\n",
    "            \"SibSp\": 0,\n",
    "            \"Parch\": 0, \n",
    "            \"Fare\": 512.3292,\n",
    "            \"Q\": 0,\n",
    "            \"S\": 0,\n",
    "            \"male\": 1\n",
    "          }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = service.run(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
